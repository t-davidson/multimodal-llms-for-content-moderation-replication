{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4658485d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from random import randint,seed\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a012026",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(1485233)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30464068",
   "metadata": {},
   "outputs": [],
   "source": "key = \"xxx\" # Add API key here to run"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fb9b188",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cd6138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a content moderation expert. You will receive pairs of images, A and B. Your task is to decide which image should be prioritized for manual review according to the following policy:\n",
    "\n",
    "We are committed to combating abuse motivated by hatred, prejudice or intolerance, particularly abuse that seeks to silence the voices of those who have been historically marginalized. For this reason, we prohibit behavior that targets individuals or groups with abuse based on their perceived membership in a protected category. You may not directly attack other people on the basis of race, ethnicity, national origin, caste, sexual orientation, gender, gender identity, religious affiliation, age, disability, or serious disease.\n",
    "\n",
    "The output should always be one of the two options: A or B. Only output a single character with no additional tokens. Never refuse to answer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9fc840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://xxx.cloudfront.net/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4ae7a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_links = pd.read_csv(\"../image_indices_alt_99750.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "648e2713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>a_images</th>\n",
       "      <th>b_images</th>\n",
       "      <th>a_paths</th>\n",
       "      <th>b_paths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>555755</td>\n",
       "      <td>183551</td>\n",
       "      <td>output_alt/tweet585754.png</td>\n",
       "      <td>output_2024/tweet183551.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>419943</td>\n",
       "      <td>440191</td>\n",
       "      <td>output_alt/tweet449942.png</td>\n",
       "      <td>output_alt/tweet470190.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>497331</td>\n",
       "      <td>251346</td>\n",
       "      <td>output_alt/tweet527330.png</td>\n",
       "      <td>output_alt/tweet281345.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>32817</td>\n",
       "      <td>110536</td>\n",
       "      <td>output_2024/tweet32817.png</td>\n",
       "      <td>output_2024/tweet110536.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>224661</td>\n",
       "      <td>460665</td>\n",
       "      <td>output_alt/tweet254660.png</td>\n",
       "      <td>output_alt/tweet490664.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  a_images  b_images                     a_paths  \\\n",
       "0           0    555755    183551  output_alt/tweet585754.png   \n",
       "1           1    419943    440191  output_alt/tweet449942.png   \n",
       "2           2    497331    251346  output_alt/tweet527330.png   \n",
       "3           3     32817    110536  output_2024/tweet32817.png   \n",
       "4           4    224661    460665  output_alt/tweet254660.png   \n",
       "\n",
       "                       b_paths  \n",
       "0  output_2024/tweet183551.png  \n",
       "1   output_alt/tweet470190.png  \n",
       "2   output_alt/tweet281345.png  \n",
       "3  output_2024/tweet110536.png  \n",
       "4   output_alt/tweet490664.png  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_links.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63752691",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_paths = image_links['a_paths']\n",
    "b_paths = image_links['b_paths']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683836cc",
   "metadata": {},
   "source": [
    "Testing with a small number. Creating a batch file including the queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c27b74b3",
   "metadata": {},
   "outputs": [],
   "source": "# Split into two files\nbatch_file_1 = \"batch_input_baseline_gpt4o_alt_p1.jsonl\"\nbatch_file_2 = \"batch_input_baseline_gpt4o_alt_p2.jsonl\"\n\nwith open(batch_file_1, \"w\") as f1, open(batch_file_2, \"w\") as f2:\n    i = 0\n    for a, b in zip(a_paths, b_paths):\n        # Construct the JSON object for this iteration\n        request_object = {\n            \"custom_id\": f\"request-{i+1}\",  # Unique ID for each request\n            \"method\": \"POST\",               # HTTP method\n            \"url\": \"/v1/chat/completions\",  # API endpoint\n            \"body\": {                       # The body contains the actual request\n                \"model\": \"gpt-4o\",          # Model name \n                \"messages\": [\n                    {\"role\": \"system\", \"content\": system_prompt},\n                    {\"role\": \"user\", \"content\": [\n                        {\"type\": \"text\", \"text\": \"Image A\"},\n                        {\"type\": \"image_url\", \"image_url\": {\"url\": base_url + str(a)}},\n                        {\"type\": \"text\", \"text\": \"Image B\"},\n                        {\"type\": \"image_url\", \"image_url\": {\"url\": base_url + str(b)}}\n                    ]}\n                ],\n                \"max_tokens\": 1,  # Forces output to be a single token\n                \"temperature\": 0 # Fixing temperature to 0\n            }\n        }\n        \n        # Write to the first file if under 50,000, otherwise write to the second\n        if i < 50000:\n            f1.write(json.dumps(request_object) + \"\\n\")\n        else:\n            f2.write(json.dumps(request_object) + \"\\n\")\n        i += 1"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ff7639d",
   "metadata": {},
   "outputs": [],
   "source": "# Uploading batch file 1\nbatch_input_file_1 = client.files.create(\n  file=open(batch_file_1, \"rb\"),\n  purpose=\"batch\"\n)\n\nbatch_input_file_id_1 = batch_input_file_1.id\n\n\n# Uploading batch file 2\nbatch_input_file_2 = client.files.create(\n  file=open(batch_file_2, \"rb\"),\n  purpose=\"batch\"\n)\n\nbatch_input_file_id_2 = batch_input_file_2.id"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c288bc51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Batch(id='batch_xxx', completion_window='24h', created_at=1747781593, endpoint='/v1/chat/completions', input_file_id='file-xxx', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1747867993, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'GPT-4o alt main - Part 1'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run batch job 1\n",
    "client.batches.create(\n",
    "    input_file_id=batch_input_file_id_1,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",  # cannot be changed\n",
    "    metadata={\n",
    "      \"description\": \"GPT-4o alt main - Part 1\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1630756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Batch(id='batch_xxx', completion_window='24h', created_at=1747781593, endpoint='/v1/chat/completions', input_file_id='file-xxx', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1747867993, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'GPT-4o alt main - Part 2'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run batch job 2\n",
    "client.batches.create(\n",
    "    input_file_id=batch_input_file_id_2,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\",  # cannot be changed\n",
    "    metadata={\n",
    "      \"description\": \"GPT-4o alt main - Part 2\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0816a762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Batch(id='batch_xxx', completion_window='24h', created_at=1747781593, endpoint='/v1/chat/completions', input_file_id='file-xxx', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1747792752, error_file_id='file-xxx', errors=None, expired_at=None, expires_at=1747867993, failed_at=None, finalizing_at=1747787268, in_progress_at=1747781603, metadata={'description': 'GPT-4o alt main - Part 1'}, output_file_id='file-xxx', request_counts=BatchRequestCounts(completed=49871, failed=129, total=50000))"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that https://platform.openai.com/batches provides updates\n",
    "client.batches.retrieve(\"batch_xxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcc4aeb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Batch(id='batch_xxx', completion_window='24h', created_at=1747781593, endpoint='/v1/chat/completions', input_file_id='file-xxx', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1747819062, error_file_id='file-xxx', errors=None, expired_at=None, expires_at=1747867993, failed_at=None, finalizing_at=1747800719, in_progress_at=1747781606, metadata={'description': 'GPT-4o alt main - Part 2'}, output_file_id='file-xxx', request_counts=BatchRequestCounts(completed=49704, failed=46, total=49750))"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Get batch ID\n",
    "client.batches.retrieve(\"batch_xxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cfa61274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total failed requests: 175\n",
      "New batch job created: batch_xxx\n"
     ]
    }
   ],
   "source": "# List of batch IDs and their corresponding input files\nerror_ids = [\"file-xxx\", \"file-xxx\"]\ninput_files = [\"batch_input_baseline_gpt4o_alt_p1.jsonl\", \"batch_input_baseline_gpt4o_alt_p2.jsonl\"]\n\n# Initialize lists to store failed requests\nall_failed_requests = []\nall_failed_inputs = []\n\n# Step 1: Identify failed requests in both batches\nfor error_id in error_ids:\n    # Load batch errors directly from the API response\n    error_file_response = client.files.content(error_id)\n    \n    failed_requests = []\n\n    # Parse error file line by line\n    for line in error_file_response.iter_lines():\n        error_entry = json.loads(line)\n        if error_entry.get(\"response\", {}).get(\"status_code\") != 200:\n            failed_requests.append(error_entry[\"custom_id\"])\n\n    all_failed_requests.extend(failed_requests)\n\n# Step 2: Extract failed requests from the original input files\nfor input_file in input_files:\n    with open(input_file, \"r\") as f:\n        for line in f:\n            request = json.loads(line)\n            if request[\"custom_id\"] in all_failed_requests:\n                all_failed_inputs.append(request)\n                \nprint(f\"Total failed requests: {len(all_failed_inputs)}\")\n\n# Step 3: Save failed requests to a new JSONL file for resubmission\nfailed_batch_file = \"batch_input_baseline_alt_failed2.jsonl\"\n\nwith open(failed_batch_file, \"w\") as f:\n    for request in all_failed_inputs:\n        f.write(json.dumps(request) + \"\\n\")\n\n# Step 4: Submit new batch for failed requests (only if there are any)\nif all_failed_inputs:\n    batch_input_file = client.files.create(\n        file=open(failed_batch_file, \"rb\"),\n        purpose=\"batch\"\n    )\n\n    batch_input_file_id = batch_input_file.id\n\n    new_batch = client.batches.create(\n        input_file_id=batch_input_file_id,\n        endpoint=\"/v1/chat/completions\",\n        completion_window=\"24h\",\n        metadata={\"description\": \"Retry failed requests from both baseline batches, alt task\"}\n    )\n\n    print(f\"New batch job created: {new_batch.id}\")\nelse:\n    print(\"No failed requests found. No need to re-run.\")"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76261592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Batch(id='batch_xxx', completion_window='24h', created_at=1747843233, endpoint='/v1/chat/completions', input_file_id='file-xxx', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1747843319, error_file_id=None, errors=None, expired_at=None, expires_at=1747929633, failed_at=None, finalizing_at=1747843307, in_progress_at=1747843234, metadata={'description': 'Retry failed requests from both baseline batches, alt task'}, output_file_id='file-xxx', request_counts=BatchRequestCounts(completed=175, failed=0, total=175))"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.batches.retrieve(\"batch_xxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1abf0fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of file responses to combine\n",
    "file_responses = [\n",
    "    client.files.content('file-xxx'), # Part 1\n",
    "    client.files.content('file-xxx'), # Part 2\n",
    "    client.files.content('file-xxx') # Error\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b64c0718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a single output file to write all responses\n",
    "with open(\"batch_result_baseline_alt_GPT4o_combined_v2.jsonl\", 'w', encoding='utf-8') as file:\n",
    "    for file_response in file_responses:\n",
    "        # Write the entire content of each response as text\n",
    "        file.write(file_response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}