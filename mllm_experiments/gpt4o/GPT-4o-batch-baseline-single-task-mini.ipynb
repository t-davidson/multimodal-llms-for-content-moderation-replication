{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4658485d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from random import randint,seed\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30464068",
   "metadata": {},
   "outputs": [],
   "source": "key = \"xxx\" # Add API key here to run"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fb9b188",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cd6138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt modified for a single task\n",
    "system_prompt = \"\"\"\n",
    "You are a content moderation expert. You will receive an image. Your task is to decide whether it should be sent for manual review according to the following policy:\n",
    "\n",
    "We are committed to combating abuse motivated by hatred, prejudice or intolerance, particularly abuse that seeks to silence the voices of those who have been historically marginalized. For this reason, we prohibit behavior that targets individuals or groups with abuse based on their perceived membership in a protected category. You may not directly attack other people on the basis of race, ethnicity, national origin, caste, sexual orientation, gender, gender identity, religious affiliation, age, disability, or serious disease.\n",
    "\n",
    "The output should always be one of the two options: Yes or No. Only output a single word with no additional tokens. Never refuse to answer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9fc840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://xxx.cloudfront.net/output/tweet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7bfc1a",
   "metadata": {},
   "source": [
    "Reading in randomized images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb9154a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_numbers = pd.read_csv(\"image_indices_30k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90472c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_images = list(image_numbers['a_images'])\n",
    "b_images = list(image_numbers['b_images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "133dd42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_images.extend(b_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1524bc6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "109ffb38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52193"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(a_images)) # There are some duplicates, but we're just going to keep them in there\n",
    "# It will be interesting to see if any are coded differently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683836cc",
   "metadata": {},
   "source": [
    "Testing with a small number. Creating a batch file including the queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c27b74b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that only 50k requests can be added so will need to do this in two parts\n",
    "with open(\"batch_input_baseline_single_part1_mini.jsonl\", \"w\") as f:\n",
    "    i = 0\n",
    "    for a in a_images:\n",
    "        if i <= 30000:\n",
    "            # Construct the JSON object for this iteration\n",
    "            request_object = {\n",
    "                \"custom_id\": f\"request-{i+1}\",  # Unique ID for each request\n",
    "                \"method\": \"POST\",               # HTTP method\n",
    "                \"url\": \"/v1/chat/completions\",  # API endpoint\n",
    "                \"body\": {                       # The body contains the actual request\n",
    "                    \"model\": \"gpt-4o-mini\",          # Model name \n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": [\n",
    "                            {\"type\": \"text\", \"text\": \"Image\"},\n",
    "                            {\"type\": \"image_url\", \"image_url\": {\"url\": base_url + str(a) + \".png\"}}\n",
    "                        ]}\n",
    "                    ],\n",
    "                    \"max_tokens\": 1,  # Yes and No both consume a single token,\n",
    "                    \"temperature\": 0\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Write each request object as a JSON line\n",
    "            f.write(json.dumps(request_object) + \"\\n\")\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26e6cf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that only 50k requests can be added so will need to do this in two parts\n",
    "with open(\"batch_input_baseline_single_part2_mini.jsonl\", \"w\") as f:\n",
    "    i = 0\n",
    "    for a in a_images:\n",
    "        if i > 30000:\n",
    "            # Construct the JSON object for this iteration\n",
    "            request_object = {\n",
    "                \"custom_id\": f\"request-{i+1}\",  # Unique ID for each request\n",
    "                \"method\": \"POST\",               # HTTP method\n",
    "                \"url\": \"/v1/chat/completions\",  # API endpoint\n",
    "                \"body\": {                       # The body contains the actual request\n",
    "                    \"model\": \"gpt-4o-mini\",          # Model name \n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": [\n",
    "                            {\"type\": \"text\", \"text\": \"Image\"},\n",
    "                            {\"type\": \"image_url\", \"image_url\": {\"url\": base_url + str(a) + \".png\"}}\n",
    "                        ]}\n",
    "                    ],\n",
    "                    \"max_tokens\": 1,  # Yes and No both consume a single token\n",
    "                    \"temperature\": 0\n",
    "                }\n",
    "            }\n",
    "\n",
    "            # Write each request object as a JSON line\n",
    "            f.write(json.dumps(request_object) + \"\\n\")\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ff7639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading batch\n",
    "batch_input_file = client.files.create(\n",
    "  file=open(\"batch_input_baseline_single_part1_mini.jsonl\", \"rb\"),\n",
    "  purpose=\"batch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b773e3c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'file-xxx'"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_input_file_id = batch_input_file.id\n",
    "batch_input_file_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55c71038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Batch(id='batch_xxx', completion_window='24h', created_at=1747521581, endpoint='/v1/chat/completions', input_file_id='file-xxx', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1747607981, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'Single eval mini - p1'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run batch job\n",
    "client.batches.create(\n",
    "    input_file_id=batch_input_file_id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\", # cannot be changed\n",
    "    metadata={\n",
    "      \"description\": \"Single eval mini - p1\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9f5b34",
   "metadata": {},
   "source": [
    "The `output_file_id` field will appear below once batches begin processing. This can be used to retrieve the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b52bc94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Batch(id='batch_xxx', completion_window='24h', created_at=1747521581, endpoint='/v1/chat/completions', input_file_id='file-xxx', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1747532131, error_file_id='file-xxx', errors=None, expired_at=None, expires_at=1747607981, failed_at=None, finalizing_at=1747526450, in_progress_at=1747521588, metadata={'description': 'Single eval mini - p1'}, output_file_id='file-xxx', request_counts=BatchRequestCounts(completed=29998, failed=3, total=30001))"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.batches.retrieve(\"batch_xxx\") # Can also be viewed in OpenAI platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c10ff4",
   "metadata": {},
   "source": [
    "Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a4ad928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading batch 2\n",
    "\n",
    "batch_input_file2 = client.files.create(\n",
    "\n",
    "  file=open(\"batch_input_baseline_single_part2_mini.jsonl\", \"rb\"),\n",
    "\n",
    "  purpose=\"batch\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f61f8654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'file-xxx'"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_input_file_id2 = batch_input_file2.id\n",
    "\n",
    "batch_input_file_id2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1777a857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Batch(id='batch_xxx', completion_window='24h', created_at=1747521590, endpoint='/v1/chat/completions', input_file_id='file-xxx', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1747607990, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'Single eval mini - p2'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run batch job\n",
    "\n",
    "client.batches.create(\n",
    "\n",
    "    input_file_id=batch_input_file_id2,\n",
    "\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "\n",
    "    completion_window=\"24h\", # cannot be changed\n",
    "\n",
    "    metadata={\n",
    "\n",
    "      \"description\": \"Single eval mini - p2\"\n",
    "\n",
    "    }\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfbbe688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Batch(id='batch_xxx', completion_window='24h', created_at=1747521590, endpoint='/v1/chat/completions', input_file_id='file-xxx', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1747532388, error_file_id='file-xxx', errors=None, expired_at=None, expires_at=1747607990, failed_at=None, finalizing_at=1747527720, in_progress_at=1747521602, metadata={'description': 'Single eval mini - p2'}, output_file_id='file-xxx', request_counts=BatchRequestCounts(completed=29925, failed=74, total=29999))"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.batches.retrieve(\"batch_xxx\") # Can also be viewed in OpenAI platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63911277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total failed requests: 77\n",
      "New batch job created: batch_xxx\n"
     ]
    }
   ],
   "source": "# List of batch IDs and their corresponding input files\nerror_ids = [\"file-xxx\", \"file-xxx\"]\ninput_files = [\"batch_input_baseline_single_part1_mini.jsonl\", \"batch_input_baseline_single_part2_mini.jsonl\"]\n\n# Initialize lists to store failed requests\nall_failed_requests = []\nall_failed_inputs = []\n\n# Step 1: Identify failed requests in both batches\nfor error_id in error_ids:\n    # Load batch errors directly from the API response\n    error_file_response = client.files.content(error_id)\n    \n    failed_requests = []\n\n    # Parse error file line by line\n    for line in error_file_response.iter_lines():\n        error_entry = json.loads(line)\n        if error_entry.get(\"response\", {}).get(\"status_code\") != 200:\n            failed_requests.append(error_entry[\"custom_id\"])\n\n    all_failed_requests.extend(failed_requests)\n\n# Step 2: Extract failed requests from the original input files\nfor input_file in input_files:\n    with open(input_file, \"r\") as f:\n        for line in f:\n            request = json.loads(line)\n            if request[\"custom_id\"] in all_failed_requests:\n                all_failed_inputs.append(request)\n                \nprint(f\"Total failed requests: {len(all_failed_inputs)}\")\n\n# Step 3: Save failed requests to a new JSONL file for resubmission\nfailed_batch_file = \"batch_input_baseline_single_mini_failed.jsonl\"\n\nwith open(failed_batch_file, \"w\") as f:\n    for request in all_failed_inputs:\n        f.write(json.dumps(request) + \"\\n\")\n\n# Step 4: Submit new batch for failed requests (only if there are any)\nif all_failed_inputs:\n    batch_input_file = client.files.create(\n        file=open(failed_batch_file, \"rb\"),\n        purpose=\"batch\"\n    )\n\n    batch_input_file_id = batch_input_file.id\n\n    new_batch = client.batches.create(\n        input_file_id=batch_input_file_id,\n        endpoint=\"/v1/chat/completions\",\n        completion_window=\"24h\",\n        metadata={\"description\": \"Retry failed requests from both baseline batches, single task mini\"}\n    )\n\n    print(f\"New batch job created: {new_batch.id}\")\nelse:\n    print(\"No failed requests found. No need to re-run.\")"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "728a3690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Batch(id='batch_xxx', completion_window='24h', created_at=1747580681, endpoint='/v1/chat/completions', input_file_id='file-xxx', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1747580724, error_file_id=None, errors=None, expired_at=None, expires_at=1747667081, failed_at=None, finalizing_at=1747580719, in_progress_at=1747580682, metadata={'description': 'Retry failed requests from both baseline batches, single task mini'}, output_file_id='file-xxx', request_counts=BatchRequestCounts(completed=77, failed=0, total=77))"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.batches.retrieve(\"batch_xxx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2260a747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of file responses to combine\n",
    "file_responses = [\n",
    "    client.files.content('file-xxx'), # Part 1\n",
    "    client.files.content('file-xxx'), # Part 2\n",
    "    client.files.content('file-xxx') # Error\n",
    "]\n",
    "\n",
    "# Open a single output file to write all responses\n",
    "with open(\"batch_result_baseline_single_GPT4o_mini_combined.jsonl\", 'w', encoding='utf-8') as file:\n",
    "    for file_response in file_responses:\n",
    "        # Write the entire content of each response as text\n",
    "        file.write(file_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282dd268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}